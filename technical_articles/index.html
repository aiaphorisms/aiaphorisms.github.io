<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Technical Articles</title>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" />
  <link rel="stylesheet" href="../css/styles.css" />
</head>
<body>
  <header>
    <div id="nav-placeholder"></div>
  </header>
  <main class="container">
    <h1>Technical Articles</h1>
    <!-- List of technical articles goes here -->
    <ul>
            <!-- Each essay can be a new list item -->
            <li>
                <a href="tech/ConciseDeepNLP.pdf">A Very Short Overview of Development of NLP in Recent Times</a>
                <p>This essay provides a brief overview of the evolution of Natural Language Processing (NLP) technologies, with a focus on recent advancements.</p>
            </li>
            <!-- Add more essays as new list items -->
            <li>
    <a href="tech/history_of_search.pdf">The Evolutionary Journey of Search Engines: Architecture and Evaluations</a>
    <p>Delve into a concise exploration tracing the history and evolution of modern search engines. This document offers insights into their underlying architecture and the pivotal methods used for their evaluation.</p>
</li>
<li>
  <a href="tech/LLMs_and_Future_of_Search.pdf">LLMs and the Future of Search</a>
  <p>Explore how Large Language Models (LLMs) are poised to revolutionize search, building upon the sophisticated AI systems and complex engineering that power today's search engines, from web crawling and indexing to query understanding and ranking.  This document examines the evolution of search and the potential impact of LLMs on its future.</p>
</li>
<li>
    <a href="tech/LLMsAndSearch.pdf">The Dawn of LLMs: A Visionary Perspective on the Future of Search</a>
    <p>Engage with a thought-provoking opinion piece, tailored for a technical audience, that envisions the transformative potential of Large Language Models in reshaping the future landscape of search.</p>
</li>
<li>
    <a href="tech/Licensing.pdf">Licensing Code: A Primer for Software Engineers</a>
    <p>A brief note on licensing softwares.</p>
</li>
 <li>
    <a href="tech/AIEthicsRegulation.pdf">Ethical Considerations and Regulations in Artificial Intelligence</a>
    <p>A brief discussion on AI ethics and necessity of regulations.</p>
</li>
<li>
    <a href="tech/AIExecutiveOrder.pdf">Summary of US government's  executive order on AI</a>
    <p>It is a quick read to understand the essence of President Biden's executive order on AI.</p>
</li>
<li>
  <a href="tech/RL_Review.pdf">A Concise Review of Reinforcement Learning Methods: Foundations, Deep RL, & LLM-centric Approaches</a>
  <p>A brief overview of reinforcement learning (RL) methods, focusing on the foundations, deep RL techniques, and their application within the context of Large Language Models (LLMs).</p>
</li>
<li>
  <a href="tech/distillation.pdf">Notes on Model Distillation</a>
  <p>A review of the literature on model distillation, exploring algorithms and applications of this important method for model compression and efficiency.</p>
</li>
<li>
  <a href="tech/Evolution_Beyond_Search.pdf">The Evolution Beyond Search – From Information Retrieval to Autonomous Execution</a>
  <p>Part 2 of the series on LLMs and the Future of Search (see also <a href="tech/LLMs_and_Future_of_Search.pdf">Part 1</a>). This article explores the progression of search beyond simple information retrieval, examining the potential for autonomous execution and the role of Large Language Models.</p>
</li>
<li>
  <a href="tech/oaianthropic.pdf">Biological Intelligence and Machine Learning: A Comparative Analysis of OpenAI and Anthropic Approaches</a>
  <p>This analysis delves into the contrasting model development strategies employed by OpenAI and Anthropic, two of the most influential AI research organizations.</p>
</li>
<li>
  <a href="tech/agenticai.pdf">Agents and Agentic AI in the Era of Large Language Models</a>
  <p>This essay provides a comprehensive overview of agentic AI, explaining what agents are, how Large Language Models facilitate their creation through capabilities like planning and tool use, and what constitutes an effective agent platform. It describes suitable LLMs, details existing frameworks (e.g., LangChain, AutoGen) and managed services (e.g., OpenAI Assistants), comparing their features, pros, and cons, while also covering implementation specifics, workflow design, and cost factors to offer readers a solid understanding of this rapidly evolving space.
</p>
</li>
<li>
  <a href="tech/SearchEvolution.pdf">Search 2025: From Information Access to Intelligent Execution</a>
<p>
In this essay, I explore how LLMs are transforming search from a user-facing interface into invisible infrastructure that enables real-time reasoning, retrieval, and execution within intelligent systems..
</p>
</li>
<li>
  <a href="tech/RevivalPlanning.pdf">The Rise of Reasoning Models: A Revival of Planning in AI</a>
<p>
From prediction to deliberation — how modern AI is rediscovering the value of structured reasoning.
</p>
</li>
<li>
  <a href="tech/data_engine_paper.pdf">Data Engines for Autonomous Vehicles:
A Reference Architecture</a>
<p>
This paper presents a reference architecture for production data engines in autonomous vehicle
systems—the closed-loop infrastructure that transforms raw sensor data for training ML models.
</p>
</li>
<li>
  <a href="tech/aisystemstrend2026.pdf">Architectural Advances in AI Through a Systems Lens
</a>
<p>
This paper examines most significant developments across the full AI stack—from model architecture and
training paradigms to compiler infrastructure, runtime systems, accelerators, interconnect,
and scientific applications—and shows that the practical impact of modern AI architectures is
constrained less by model design than by the maturity of the execution stack beneath them.
</p>
</li>
<li>
  <a href="tech/AIForEnterprises2026.pdf">
    Enterprise AI Outlook 2026 and Beyond: Applications, Inference Optimization, and Infrastructure
  </a>
  <p>
    This white paper examines the transition of AI from model-centric experimentation to
    enterprise-scale application and system deployment. It analyzes trends across training,
    inference optimization, agentic application design, hardware infrastructure, data strategy,
    and evaluation, and argues that beyond 2026 the primary constraints on AI impact are economic,
    architectural, and operational rather than model capability alone.
  </p>
</li>
<li>
  <a href="tech/GPUInfrastructureOutlook2026.pdf">
    GPU Cluster Infrastructure Outlook 2026 and Beyond: Power, Compute, and Strategic Constraints
  </a>
  <p>
    This white paper analyzes the global GPU cluster infrastructure inflection driven by AI-scale
    workloads. It examines market expansion, hyperscaler capex, geographic concentration, accelerator
    competition, power and cooling constraints, memory and interconnect limits, and supply-chain
    dynamics, arguing that through 2028 the binding constraints on AI progress are infrastructure
    readiness, energy availability, and system-level reliability rather than silicon performance alone.
  </p>
</li>

<li>
  <a href="tech/AdobeDocumentCloudDisruptionThesis.pdf">
    The Adobe Document Cloud Disruption Thesis: PDF Wins, But Does Adobe? (2025–2035)
  </a>
  <p>
    This analysis examines the paradox facing Adobe's $3.18B Document Cloud business: PDF succeeds
    as the universal document format while Adobe faces disintermediation at every value-creating
    layer. It maps programmatic generation bypass, e-signature commoditization, and AI-native
    document intelligence capture by startups like Reducto and Unstructured, arguing that 35–50%
    of Document Cloud's addressable market is vulnerable by 2035—unless Adobe executes an
    aggressive AI-native pivot through acquisition or "Firefly for Documents" capabilities.
  </p>
</li>

<li>
  <a href="tech/AIAdEnginesTransformation2026.pdf">
    The Transformation of Ad Engines: LLMs, Foundation Models, and Agentic Systems (2026+)
  </a>
  <p>
    This analysis examines digital advertising's most significant transformation since programmatic
    buying, as LLMs, foundation models, and autonomous agents reshape every component of the ad
    stack. It details Meta's GEM achieving 5% conversion lifts, Google's AI Overviews reaching
    1.5B users with integrated ads, and the collapse of organic CTR from 7.3% to 2.6%. The report
    argues that while AI enables unprecedented efficiency, a countervailing consumer reckoning
    looms—58% replacing search with AI, 50% limiting social media usage, and only one-third
    viewing AI advertising positively—forcing platforms to balance technological capability
    against eroding consumer trust.
  </p>
</li>

<li>
  <a href="tech/StateOfAISilicon2026.pdf">
    State of AI Silicon 2026: The Definitive Technical Atlas
  </a>
  <p>
This technical report maps the GPU design landscape through the engineering constraints that now matter more than raw compute: memory bandwidth, packaging capacity, and software ecosystem maturity. It compares NVIDIA's dual-die Blackwell (208B transistors, 8 TB/s), AMD's chiplet MI300X (33% TCO advantage for memory-bound inference), Cerebras's wafer-scale 21 PB/s on-chip bandwidth, and China's Ascend 910C reaching 60% H100 performance through system-scale brute force. The analysis covers supply chain bottlenecks (HBM sold out through 2026, CoWoS at 60% NVIDIA allocation), the CUDA moat (6M developers vs. ROCm/CANN fragmentation), and market dynamics as inference spending overtakes training—projecting value migration from NVIDIA's 86–94% training monopoly toward a fragmented $254B inference market by 2030.
  </p>
</li>
<li>
  <a href="tech/ImageReasoningGenerativeModels2026.pdf">
    Image Reasoning and Generative Models: A First-Principles Survey
  </a>
  <p>
This survey traces image generation from foundational theory through frontier systems, explaining why specific architectures emerged rather than merely cataloging them. It covers the three dominant paradigms—diffusion models (DDPM, latent diffusion, classifier-free guidance), rectified flow (SD3, FLUX), and autoregressive approaches (LlamaGen, VAR, MaskGIT)—alongside critical components: visual tokenizers, text encoders (CLIP, T5, native multimodal), and conditioning mechanisms (ControlNet, IP-Adapter). The analysis spans landmark commercial systems (DALL-E, GPT-Image, Gemini/Nano Banana, Qwen-Image), unified understanding-generation architectures (Chameleon, Emu3, Transfusion), and structured generation via scene graphs and programmatic synthesis. Practical sections cover training infrastructure (LAION-5B, scaling laws, distributed training), evaluation frameworks (FID limitations, T2I-CompBench, GENEVAL, learned preference metrics), and persistent weaknesses—compositional reasoning failures at 15–54% accuracy, hand/text rendering gaps, and the demo-to-deployment reliability chasm. Future projections include the obsolescence of separate "language" and "image" models by 2027, rectified flow displacing DDPM, and mobile deployment becoming viable as distillation reduces generation from 1000 steps to 1–4.
  </p>
</li>
<li>
  <a href="tech/AIMemoryArchitectureLLMs.pdf">
    AI Memory Architecture for Large Language Models: From Context Windows to Persistent Intelligence
  </a>
  <p>
    This technical survey examines memory architectures for LLMs through the 3D-8Q taxonomy (object, form, time dimensions), mapping how systems store and retrieve knowledge across eight distinct paradigms. It compares neural memory modules—Google's Titans achieving 2M+ token contexts through test-time memorization, Meta's Memory Layers adding 128B parameters with 168% QA accuracy gains—against retrieval-augmented approaches like HippoRAG's graph-based multi-hop reasoning. The analysis covers KV cache optimization (PagedAttention's 96% utilization, FlashAttention's O(N) memory scaling, H2O's 29× throughput via heavy-hitter eviction), GPU memory hierarchies from registers to HBM3e, and NVIDIA's open-source infrastructure (TensorRT-LLM, Dynamo's distributed KV management). Production benchmarks show Mem0 delivering 26% accuracy improvement with 91% latency reduction, while Mooncake's disaggregated architecture processes 100B+ tokens daily—signaling memory has become the critical bottleneck as context windows scale toward millions of tokens.
  </p>
</li>

        </ul>
  </main>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  <script>
    $(function(){
      $("#nav-placeholder").load("../includes/nav.html", function(response, status, xhr) {
        if(status === "error") {
          console.error("Error loading navigation:", xhr.status, xhr.statusText);
        }
      });
    });
  </script>
</body>
</html>

